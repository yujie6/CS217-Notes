\documentclass[12pt,a4]{article}

\usepackage{xcolor}


\newcommand{\handoutdate}{Friday, 2020-06-05}
\newcommand{\firstduedate}{Thursday, 2020-06-12}
\newcommand{\finalduedate}{Thursday, 2020-06-19}


\input{preamble}




\newcommand{\rank}{\textnormal{rank}}
\newcommand{\y}{\mathbf{y}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\V}{\mathbf{v}}

\renewcommand{\a}{\mathbf{a}}

\renewcommand{\b}{\mathbf{b}} 
\newcommand{\zero}{\mathbf{0}}
\newcommand{\rpn}{\mathbb{R}_{\geq 0}}
\newcommand{\sol}{\textup{\textrm{sol}}}
\newcommand{\opt}{\textup{\textrm{opt}}}
\setcounter{section}{6}


\section{Farkas Lemma and LP Duality}

\subsection{Different Versions of Farkas Lemma}

In the following, let $A \in \R^{m \times n}$ and $\b \in \R^m$, and let 
$\x = (x_1,\dots,x_n)^T$ be a column vector of $n$ variables and 
$\y = (y_1, \dots,y_m)$ be a row vector of $m$ variables.

\begin{exercise}
 Show that the three versions of Farkas Lemma presented in class are all equivalent (I actually did not present
 the third version in class):
 \begin{align}
   ( \neg \exists \x : \, A \x \leq \b ) \, & \Longleftrightarrow 
    ( \exists \y \geq \zero : \, \y^T A = \zero, \,     \y^T \b < 0 ) \ . \\
      ( \neg \exists \x \geq \zero : \, A \x \leq \b ) \, & \Longleftrightarrow 
    ( \exists \y \geq \zero : \, \y^T A \geq \zero, \,  \y^T \b < 0 ) \ . \\
   ( \neg \exists \x \geq \zero : \, A \x = \b ) \, & \Longleftrightarrow 
    ( \exists \y \begingroup \color{white} \geq \zero \endgroup : \, \y^T A \geq \zero, \,  \y^T \b < 0 ) \ .
 \end{align}
  Note that the direction ``$\Longleftarrow$'' is easy in each case. 
  We will show the ``$\Longrightarrow$'' of (1) in class using a technique called {\em Fourier-Motzkin Elimination}. 
  This exercise is actually not that hard. The hardest part is keeping track of what you 
  want to prove and what you can assume.
\end{exercise}
\begin{proof}
		First we prove (1) and (2) are equivalent. First consider (1) $\implies$ (2). If $\neg \exists \x \ge  \zero : A\x \le \b$, 
		notice that $\x \ge \zero $ iff $\left( -I_{n} \right) \x \le \zero$ , we can construct a new maxtrix $A'$ 
		\[
		A' = \begin{pmatrix} -I \\ A \end{pmatrix} , \b' = \begin{pmatrix} \zero\\ \b \end{pmatrix} 
		.\] 
		such that our assumption turns into
		\[
		\neg \exists \x : A' \x \le \b'
		.\] 
		And by (1), if follows that $\exists \y ' \ge \zero$ s.t. $(\y ')^{\top} A' = 0$ and  $(\y ') ^{\top} \b ' < 0$.
		For convenience, we wirte $\y' = \left( z_1,z_2\ldots,z_n, \y \right) $ while $z_{i} \ge 0$, therefore
		\[
		-z_{i} + \y ^{\top} A_{i} = 0 \implies \y^{\top} A_{i} = z_{i} \ge  0
		.\] 
		Which leads that $\y^{\top} A \ge 0$.
		Similarly $(\y')^{\top} \b'= \y ^{\top} \b  < 0 $, thus (2) is true. 
		
		Next we prove (2) $\implies$ (3). If $\neg \x \ge 0, A\x = \b$, which is $A \x \le \b$ plus $\left( -A \right) \x \le -\b$,
		similarly we construct 
		\[
		A' = \begin{pmatrix} A \\ -A \end{pmatrix}, \b' = \begin{pmatrix} \b \\ -\b \end{pmatrix}  
		.\] 
		By (2), $\exists \y' = \left( \y_1, \y_2 \right) $ , such that 
		\[
		\y_1 A - \y_2 A \ge 0, \y_1\b - \y_2 \b < 0
		.\] 
		Choose $\y = \y_1 - \y_2$ we are done.

		Now we prove (2) $\implies$ (1). 
		
		We know that $\neg \x \ge 0, A\left( \x \right) \le \b$.
		And it's also obvious that $\neg \exists \x \ge 0, A \left( -\x \right) = \left( -A \right) \x\le \b$, 
		we can build a new matrix $A' = \left( A, -A \right) $, hence 
		\[
				\neg \x' = \begin{pmatrix} \x_1\\ \x_2 \end{pmatrix}  \ge 0, \left( A, -A \right) \begin{pmatrix} \x_1 \\ \x_2 \end{pmatrix} \le 2\b
		.\] 
		By (2) we have 
		\[
				\exists \y, \y^{\top} A \ge 0, \y ^{\top} \left( -A \right) \ge 0, \y^{\top} \b < 0 \implies \y^{\top} A = 0
		.\] 
		Thus (2) $\implies$ (1) is true.

		Finally we prove (3) $\implies$ (2), if $\neg \exists \x \ge 0: A\x \le \b$, similarly construct
		\[
				A' = \left( A, I \right) 
		.\] 
		We know $\neg \exists \x \ge 0, A' \x' = \b$.
		The result follows directly from (3).
\end{proof}

\subsection{A Linear Program for, well, for what?}




Let $G = (V,E)$ be a directed graph, $s,t \in V$,  and $c: E \rightarrow \mathbf{R}^+$ be a cost 
function. We want to find an $s-t$-flow $f$ of value $1$. Every edge $e$ generates cost $f(e) \cdot c(e)$, and we want to minimize the overall cost. There are no capacity constraints.
We can easily write this as a linear program MCF (Minimum Cost Flow):
\begin{align*}
  \textrm{MCF}(G,s,t,c): \qquad
  \begin{array}{ll}
    \textnormal{minimize} \quad & \multicolumn{1}{l}{\sum_{e \in E} c(e) f(e)} \\
    \\
    \textnormal{subject to} \quad & \sum_{v \in V} f(v,t)  = 1 \\
					        & \sum_{u \in V} f(u,v) - \sum_{w \in V} f(v,w)  = 0  \quad \forall\ v \in V  \setminus \{s,t\}\\
					        \\
     & f(e)  \geq 0 \ \forall \ e \in E 
  \end{array}
\end{align*}
Note that we have $m$ variables, one variable $f(e)$ for each edge $e$.
The first constraint says that the value of the flow should be 1. The other constraints say that 
the inflow at $v$ should equal the outflow.

\begin{exercise}
   Let $d$ be the shortest path distance from $s$ to $t$ in the directed graph $G$, where distance
   means sum of the $c(e)$ along the path. Show that $\opt(MCF) = d$.
   \textbf{Hint.} Make sure you show both $\leq$ and $\geq$.
\end{exercise}

\begin{proof}
To prove $opt(MCF) \le d$, we just need to prove that the shortest path is a solution to $MCF$.
    We set $f(e) = 1$ along all edges in the shortest path, since there is only one path with flow $1$,
    The constraints are obviously satisfied. So it is a solution of $MCF$, and its value is $1 * d = d$,
    so $opt(MCF) \le d$

    To prove $opt(MCF) \ge d$, we need to prove that all solutions of $MCF$ is not better than $d$.
    We try to improve the value of all possible solutions to $d$.
    
    Suppose we have a solution with $x$ different $s-t$ path. Define $b(path)$ be the smallest flow in all
    edges of $path$, $d(path)$ be the length of $path$. Let $sp$ be the shortest $s-t$ path. We do as follows, choose any path $p$ besides $sp$,
    put $b(p)$ units of flow on $p$ to $sp$. Repeat it until there is only $1$ unit flowing through $sp$.

    We need to show in each turn, $val(MCF)$ is not worse than previous and no constraints are broke. We fist prove 
    $val(MCF)$ is not worse. In each turn, $val(MCF)' = val(MCF) + d * b(p) - d(p) * b(p)$, $d \le d(p)$, so $val(MCF)' \le val(MCF)$.
    As for the constraints, inflow of $t$ remains to be 1 since we just move $b(p)$ units between two different paths.
    Flow constraints remains since we modify the flow in one path, which means we move inflow and outflow of a single 
    vertex at the same time. Since we only have $x$ different $s-t$ path, and the flow value on each path is finite,
    the process terminates. So $val(MCF) \ge d$

    In all $val(MCF) = d$
\end{proof}

\begin{exercise}
    Write down the dual of MCF. This will be a maximization problem. Don't use any matrix notation.
\end{exercise}

\begin{proof}
We introduce a dual coefficient $g_v, v \in V$. The dual program is:
    \begin{itemize}
        \item Maximize $g_t$, subject to:
        \item $g_v - g_u \le c(u, v), \forall (u,v) \in E$
        \item $g_v \in \mathbb{R}, v \in V$
    \end{itemize}
\end{proof}

\begin{exercise}
   Interpret the dual. Show that it is the LP formulation of a ``natural'' maximization problem on $G$.
\end{exercise}

\begin{proof}
If we set $g_s = 0$, then the $g_v$ can be thought as the cost of some $s-t$-path.
    Since each edge $(u,v)$ must satisfy $g_v - g_u \le c(u,v))$, we can not just choose the
    maximal $s-t$-path as solution. Under this constraint, we can see that the solution
    must at first be a \textbf{safe} path, so the program is actually the shortest path problem.
\end{proof}

\begin{exercise}
  Describe an optimal solution of the dual program.
\end{exercise}

\begin{proof}
    The optimal solution is the shortest $s-t$-path $sp$, and for each vertex along the path, we must 
    set $g_v = g_u$ for $(u,v), u \in sp, v \notin sp$ accordingly to $g_v-g_u$ constraints.
\end{proof}
   
   
   
  


\end{document}
